{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias in Image based Automatic Gender Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent studies have shown that the machine learning models for gender classification task from face images perform differently across groups defined by skin tone. In this tutorial, we will demonstrate the use of the aif360 toolbox to study the the differential performance of a custom classifier. We use a bias mitigiating algorithm available in aif360 with the aim of improving a classfication model in terms of the fairness metrics. We will work with the UTK dataset for this tutorial. This can be downloaded from here:\n",
    "https://susanqq.github.io/UTKFace/\n",
    "\n",
    "In a nutshell, we will follow these steps:\n",
    " - Process images and load them as a aif360 dataset\n",
    " - Learn a baseline classifier and obtain fairness metrics\n",
    " - Call the `Reweighing` algorithm to obtain obtain instance weights\n",
    " - Learn a new classifier with the instance weights and obtain updated fairness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import numpy as np_test\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f48cfa0d970>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(99)\n",
    "np_test.random.seed(99)\n",
    "torch.manual_seed(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load and Process Images\n",
    "The first step is to to download the images `Aligned&cropped` images at this location mentioned above.\n",
    "\n",
    "After unzipping the downloaded file, point the location of the folder in the `image_dir` variable below. \n",
    "The file name has the following format `age`-`gender`-`race`-`date&time`.jpg\n",
    "\n",
    "    age: indicates the age of the person in the picture and can range from 0 to 116.\n",
    "    gender: indicates the gender of the person and is either 0 (male) or 1 (female).\n",
    "    race: indicates the race of the person and can from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern).\n",
    "    date&time: indicates the date and time an image was collected in the UTK dataset.\n",
    "\n",
    "For this tutorial we will restict the images to contrain `White` and `Others` races. We need to specify the unprivileged and previledged groups to obtain various metrics from the aif360 toolbox. We set `White` as the previledged group and `Others` as the unpreviledged group for computing the results with gender as the outcome variable that need to be predicted. We set prediction as `female (1)` as the unfavorable label and `male (0)` as favorable label for the purpose of computing metrics and does not have any special meaning in the context of gender prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "races_to_consider = [0,1,2,3,4,5]\n",
    "unprivileged_groups = [{'race': 1.0}]\n",
    "privileged_groups = [{'race': 0.0}, {'race': 2.0}, {'race': 3.0}, {'race': 4.0}, {'race': 5.0}]\n",
    "favorable_label = 0.0 \n",
    "unfavorable_label = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the `image_dir` with the downloaded and extracted images location and specify the desired image size.\n",
    "The images and loaded and resized usign opencv library. The following code creates three key numpy arrays each containing the raw images, the race attributes and the gender labels.\n",
    "\n",
    "Edit: We create a seperate image directory for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_dir = '/home/patrick/GitKraken/AIF360-master/aif360/data/UTKFace_16000_Down/'\n",
    "image_dir_test = '/home/patrick/GitKraken/AIF360-master/aif360/data/UTKFace_test/normal/'\n",
    "img_size = 64\n",
    "img_size_test = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/patrick/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "protected_race = []\n",
    "outcome_gender = []\n",
    "feature_image = []\n",
    "feature_age = []\n",
    "\n",
    "for i, image_path in enumerate(glob.glob(image_dir + \"*.jpg\")):\n",
    "    try:\n",
    "        age, gender, race = image_path.split('/')[-1].split(\"_\")[:3]\n",
    "        age = int(age)\n",
    "        gender = int(gender)\n",
    "        race = int(race)\n",
    "        \n",
    "        if race in races_to_consider:\n",
    "            protected_race.append(race)\n",
    "            outcome_gender.append(gender)\n",
    "            feature_image.append(resize(io.imread(image_path), (img_size, img_size, 3 )))\n",
    "            feature_age.append(age)  \n",
    "            \n",
    "    except:\n",
    "        print(\"Missing: \" + image_path)\n",
    "\n",
    "feature_image_mat = np.array(feature_image)\n",
    "outcome_gender_mat =  np.array(outcome_gender)\n",
    "protected_race_mat =  np.array(protected_race)\n",
    "age_mat = np.array(feature_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/patrick/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Image: 30\n"
     ]
    }
   ],
   "source": [
    "protected_race_test = []\n",
    "outcome_gender_test = []\n",
    "feature_image_test = []\n",
    "feature_age_test = []\n",
    "\n",
    "\n",
    "for i, image_path in enumerate(glob.glob(image_dir_test + \"*.jpg\")):\n",
    "    try:\n",
    "        age, gender, race = image_path.split('/')[-1].split(\"_\")[:3]\n",
    "        age = int(age)\n",
    "        gender = int(gender)\n",
    "        race = int(race)\n",
    "        \n",
    "        if race in races_to_consider:\n",
    "            protected_race_test.append(race)\n",
    "            outcome_gender_test.append(gender)\n",
    "            feature_image_test.append(resize(io.imread(image_path), (img_size, img_size, 3)))\n",
    "            feature_age_test.append(age)\n",
    "        else: print(\"Missing: \" + image_path)\n",
    "        \n",
    "    except:\n",
    "        print(\"Missing: \" + image_path)\n",
    "\n",
    "feature_image_mat_test = np_test.array(feature_image_test)\n",
    "outcome_gender_mat_test =  np_test.array(outcome_gender_test)\n",
    "protected_race_mat_test =  np_test.array(protected_race_test)\n",
    "age_mat_test = np_test.array(feature_age_test)\n",
    "\n",
    "print(\"Feature Image: \" + str(len(feature_image_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Learn a Baseline Classifier\n",
    "Lets build a simple convolutional neural network (CNN) with $3$ convolutional layers and $2$ fully connected layers using the `pytorch` framework. \n",
    "![CNN](images/cnn_arch.png)\n",
    "Each convolutional layer is followed by a maxpool layer. The final layer provides the logits for the binary gender predicition task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ThreeLayerCNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Input: 128x128 face image (eye aligned).\n",
    "    Output: 1-D tensor with 2 elements. Used for binary classification.\n",
    "    Parameters:\n",
    "        Number of conv layers: 3\n",
    "        Number of fully connected layers: 2       \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ThreeLayerCNN,self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3,6,5)\n",
    "        self.pool = torch.nn.MaxPool2d(2,2)\n",
    "        self.conv2 = torch.nn.Conv2d(6,16,5)\n",
    "        self.conv3 = torch.nn.Conv2d(16,16,6)\n",
    "        self.fc1 = torch.nn.Linear(16*4*4,120)\n",
    "        self.fc2 = torch.nn.Linear(120,2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv3(x)))\n",
    "        x = x.view(-1,16*4*4)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into train and test\n",
    "\n",
    "Let us rescale the pixels to lie between $-1$ and $1$ and split the complete dataset into train and test sets. \n",
    "We use $70$-$30$ percentage for train and test, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 16000\n",
      "N-Test: 30\n"
     ]
    }
   ],
   "source": [
    "feature_image_mat_normed = 2.0 *feature_image_mat.astype('float32') - 1.0\n",
    "feature_image_mat_normed_test = 2.0 *feature_image_mat_test.astype('float32') - 1.0\n",
    "\n",
    "N = len(feature_image_mat_normed)\n",
    "N_test = len(feature_image_mat_normed_test)\n",
    "\n",
    "print(\"N: \" + str(N))\n",
    "print(\"N-Test: \" + str(N_test))\n",
    "\n",
    "ids = np.random.permutation(N)\n",
    "ids_test = np.random.permutation(N_test)\n",
    "\n",
    "train_size= N\n",
    "test_size = N_test\n",
    "\n",
    "X_train = feature_image_mat_normed[ids[0:train_size]]\n",
    "y_train = outcome_gender_mat[ids[0:train_size]]\n",
    "X_test = feature_image_mat_normed_test[ids_test[0:test_size]]\n",
    "y_test = outcome_gender_mat_test[ids_test[0:test_size]]\n",
    "\n",
    "p_train = protected_race_mat[ids[0:train_size]]\n",
    "p_test = protected_race_mat_test[ids_test[0:test_size]]\n",
    "\n",
    "age_train = age_mat[ids[0:train_size]]\n",
    "age_test = age_mat_test[ids_test[0:test_size]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Next, we will create the pytorch train and test data loaders after transposing and converting the images and labels. The batch size is set to $64$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "X_train = X_train.transpose(0,3,1,2)\n",
    "X_test = X_test.transpose(0,3,1,2)\n",
    "\n",
    "train = torch.utils.data.TensorDataset(Variable(torch.FloatTensor(X_train.astype('float32'))), Variable(torch.LongTensor(y_train.astype('float32'))))\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test = torch.utils.data.TensorDataset(Variable(torch.FloatTensor(X_test.astype('float32'))), Variable(torch.LongTensor(y_test.astype('float32'))))\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Plain Model\n",
    "In the next few steps, we will create and intialize a model with the above described architecture and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 60, 60]             456\n",
      "         MaxPool2d-2            [-1, 6, 30, 30]               0\n",
      "            Conv2d-3           [-1, 16, 26, 26]           2,416\n",
      "         MaxPool2d-4           [-1, 16, 13, 13]               0\n",
      "            Conv2d-5             [-1, 16, 8, 8]           9,232\n",
      "         MaxPool2d-6             [-1, 16, 4, 4]               0\n",
      "            Linear-7                  [-1, 120]          30,840\n",
      "            Linear-8                    [-1, 2]             242\n",
      "================================================================\n",
      "Total params: 43,186\n",
      "Trainable params: 43,186\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 0.32\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 0.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = ThreeLayerCNN().to(device)\n",
    "summary(model, (3,img_size,img_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "Next,  we will train the model summarized above. `num_epochs` specifies the number of epochs used for  training. The learning rate is set to $0.001$. We will use the `Adam` otimizer to minimze the standard cross-entropy loss for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/250], Loss: 0.6237\n",
      "Epoch [1/5], Step [200/250], Loss: 0.6079\n",
      "Epoch [2/5], Step [100/250], Loss: 0.5269\n",
      "Epoch [2/5], Step [200/250], Loss: 0.3988\n",
      "Epoch [3/5], Step [100/250], Loss: 0.4973\n",
      "Epoch [3/5], Step [200/250], Loss: 0.5440\n",
      "Epoch [4/5], Step [100/250], Loss: 0.4607\n",
      "Epoch [4/5], Step [200/250], Loss: 0.4765\n",
      "Epoch [5/5], Step [100/250], Loss: 0.3336\n",
      "Epoch [5/5], Step [200/250], Loss: 0.4849\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "print_freq = 100\n",
    "\n",
    "# Specify the loss and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start training the model\n",
    "num_batches = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (idx+1) % print_freq == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' .format(epoch+1, num_epochs, idx+1, num_batches, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure Fairness Metrics\n",
    "Let's get the predictions of this trained model on the test and use them to compute various fariness metrics available in the aif360 toolbox. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 1 0 0 0 0 0 4 1 0 4 3 3 0 4 0 1 2 4 0 2 1 3 0 1 0 0 2 0]\n",
      "[0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0]\n",
      "[0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Run model on test set in eval mode.\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred += predicted.tolist()\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print (str(p_test))\n",
    "print (str(y_test))\n",
    "print (str(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper function defined below can be used to convert the numpy arrays and the related meta data into a aif360 dataset. This will ease the process of computing metrics and comparing two datasets. The wrapper consumes the outcome array, the protected attribute array, information about unprivileged_groups and privileged_groups; and the favorable and unfavorable label to produce an instance of aif360's `BinaryLabelDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dataset_wrapper(outcome, protected, unprivileged_groups, privileged_groups,\n",
    "                          favorable_label, unfavorable_label):\n",
    "    \"\"\" A wraper function to create aif360 dataset from outcome and protected in numpy array format.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data=outcome,\n",
    "                      columns=['outcome'])\n",
    "    df['race'] = protected\n",
    "    \n",
    "    dataset = BinaryLabelDataset(favorable_label=favorable_label,\n",
    "                                       unfavorable_label=unfavorable_label,\n",
    "                                       df=df,\n",
    "                                       label_names=['outcome'],\n",
    "                                       protected_attribute_names=['race'],\n",
    "                                       unprivileged_protected_attributes=unprivileged_groups)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_traning_dataset = dataset_wrapper(outcome=y_train, protected=p_train, \n",
    "                                                 unprivileged_groups=unprivileged_groups, \n",
    "                                                 privileged_groups=privileged_groups,\n",
    "                                                 favorable_label=favorable_label,\n",
    "                                          unfavorable_label=unfavorable_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_test_dataset = dataset_wrapper(outcome=y_test, protected=p_test, \n",
    "                                              unprivileged_groups=unprivileged_groups, \n",
    "                                              privileged_groups=privileged_groups,\n",
    "                                                 favorable_label=favorable_label,\n",
    "                                          unfavorable_label=unfavorable_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_predictions_test_dataset = dataset_wrapper(outcome=y_pred, protected=p_test, \n",
    "                                                       unprivileged_groups=unprivileged_groups,\n",
    "                                                       privileged_groups=privileged_groups,\n",
    "                                                 favorable_label=favorable_label,\n",
    "                                          unfavorable_label=unfavorable_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining the Classification Metrics\n",
    "We use the `ClassificationMetric` class from the aif360 toolbox for computing metrics based on two BinaryLabelDatasets. The first dataset is the original one and the second is the output of the classification transformer (or similar). Later on we will use `BinaryLabelDatasetMetric` which computes based on a single `BinaryLabelDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 1 0 0 0 0 0 4 1 0 4 3 3 0 4 0 1 2 4 0 2 1 3 0 1 0 0 2 0]\n",
      "[0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0]\n",
      "[0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "classified_metric_nodebiasing_test = ClassificationMetric(original_test_dataset, \n",
    "                                                 plain_predictions_test_dataset,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "\n",
    "print (str(p_test))\n",
    "print (str(y_test))\n",
    "print (str(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.866667\n",
      "Test set: Balanced classification accuracy = 0.866667\n",
      "Test set: Statistical parity difference = 0.120000\n",
      "Test set: Disparate impact = 1.250000\n",
      "Test set: Equal opportunity difference = 0.166667\n",
      "Test set: Average odds difference = 0.006410\n",
      "Test set: Theil index = 0.092420\n",
      "Test set: False negative rate difference = -0.166667\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Statistical parity difference = %f\" % classified_metric_nodebiasing_test.statistical_parity_difference())\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
    "print(\"Test set: False negative rate difference = %f\" % classified_metric_nodebiasing_test.false_negative_rate_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Apply the Reweighing algorithm to tranform the dataset\n",
    "Reweighing is a preprocessing technique that weights the examples in each (group, label) combination differently to ensure fairness before classification [1]. This is one of the very few pre-processing method we are aware of that could tractably be applied to multimedia data (since it does not work with the features).\n",
    "\n",
    "    References:\n",
    "    [1] F. Kamiran and T. Calders,\"Data Preprocessing Techniques for Classification without Discrimination,\" Knowledge and Information Systems, 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "               privileged_groups=privileged_groups)\n",
    "RW.fit(original_traning_dataset)\n",
    "transf_traning_dataset = RW.transform(original_traning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(original_traning_dataset, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "metric_tranf_train = BinaryLabelDatasetMetric(transf_traning_dataset, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between privileged and unprivileged groups = -0.008158\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Transformed training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between privileged and unprivileged groups = -0.000000\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between privileged and unprivileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "display(Markdown(\"#### Transformed training dataset\"))\n",
    "print(\"Difference in mean outcomes between privileged and unprivileged groups = %f\" % metric_tranf_train.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between privileged and unprivileged groups = 0.120000\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Transformed testing dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between privileged and unprivileged groups = 0.127884\n"
     ]
    }
   ],
   "source": [
    "metric_orig_test = BinaryLabelDatasetMetric(original_test_dataset, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "transf_test_dataset = RW.transform(original_test_dataset)\n",
    "metric_transf_test = BinaryLabelDatasetMetric(transf_test_dataset, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original testing dataset\"))\n",
    "print(\"Difference in mean outcomes between privileged and unprivileged groups = %f\" % metric_orig_test.mean_difference())\n",
    "display(Markdown(\"#### Transformed testing dataset\"))\n",
    "print(\"Difference in mean outcomes between privileged and unprivileged groups = %f\" % metric_transf_test.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Learn a New Classfier using the Instance Weights\n",
    "We can see that the reweighing was able to reduce the difference in mean outcomes between privileged and unprivileged groups. This was done by learning appropriate weights for each training instance. In the current step, we will use these learned instance weights to train a network. We  will create coustom pytorch loss called `InstanceWeighetedCrossEntropyLoss` that uses the instances weights to produce the loss value for a batch of data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tranf_train = torch.utils.data.TensorDataset(Variable(torch.FloatTensor(X_train.astype('float32'))),\n",
    "                                             Variable(torch.LongTensor(transf_traning_dataset.labels.astype('float32'))),\n",
    "                                            Variable(torch.FloatTensor(transf_traning_dataset.instance_weights.astype('float32'))),)\n",
    "tranf_train_loader = torch.utils.data.DataLoader(tranf_train, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class InstanceWeighetedCrossEntropyLoss(nn.Module):\n",
    "    \"\"\"Cross entropy loss with instance weights.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(InstanceWeighetedCrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, target, weights):\n",
    "        loss = log_sum_exp(logits) - select_target_class(logits, target.squeeze(1))\n",
    "        loss = loss * weights\n",
    "        return loss.mean()\n",
    "\n",
    "#Helper functions\n",
    "def select_target_class(logits, target):\n",
    "    batch_size, num_classes = logits.size()\n",
    "    mask = torch.autograd.Variable(torch.arange(0, num_classes)\n",
    "                                               .long()\n",
    "                                               .repeat(batch_size, 1)\n",
    "                                               .to(device)\n",
    "                                               .eq(target.data.repeat(num_classes, 1).t()))\n",
    "    return logits.masked_select(mask)\n",
    "\n",
    "def log_sum_exp(x):\n",
    "    c, _ = torch.max(x, 1)\n",
    "    y = c + torch.log(torch.exp(x - c.unsqueeze(dim=1).expand_as(x)).sum(1))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tranf_model = ThreeLayerCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/250], Loss: 0.4997\n",
      "Epoch [1/5], Step [200/250], Loss: 0.4999\n",
      "Epoch [2/5], Step [100/250], Loss: 0.5708\n",
      "Epoch [2/5], Step [200/250], Loss: 0.5397\n",
      "Epoch [3/5], Step [100/250], Loss: 0.4095\n",
      "Epoch [3/5], Step [200/250], Loss: 0.5255\n",
      "Epoch [4/5], Step [100/250], Loss: 0.4921\n",
      "Epoch [4/5], Step [200/250], Loss: 0.3849\n",
      "Epoch [5/5], Step [100/250], Loss: 0.4284\n",
      "Epoch [5/5], Step [200/250], Loss: 0.4093\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "print_freq = 100\n",
    "\n",
    "# Specify the loss and the optimizer\n",
    "criterion = InstanceWeighetedCrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(tranf_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start training the new model\n",
    "num_batches = len(tranf_train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, (images, labels, weights) in enumerate(tranf_train_loader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = tranf_model(images)\n",
    "        loss = criterion(outputs, labels, weights)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (idx+1) % print_freq == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' .format(epoch+1, num_epochs, idx+1, num_batches, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 1 0 0 0 0 0 4 1 0 4 3 3 0 4 0 1 2 4 0 2 1 3 0 1 0 0 2 0]\n",
      "[0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0]\n",
      "[0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "tranf_model.eval()\n",
    "y_pred_transf = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = tranf_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred_transf += predicted.tolist()\n",
    "y_pred_transf = np.array(y_pred_transf)\n",
    "\n",
    "print (str(p_test))\n",
    "print (str(y_test))\n",
    "print (str(y_pred_transf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us repeat the same steps as before to convert the predictions into aif360 dataset and obtain various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf_predictions_test_dataset = dataset_wrapper(outcome=y_pred_transf, protected=p_test, \n",
    "                                                  unprivileged_groups=unprivileged_groups,\n",
    "                                                  privileged_groups=privileged_groups,\n",
    "                                                 favorable_label=favorable_label,\n",
    "                                                  unfavorable_label=unfavorable_label\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_metric_debiasing_test = ClassificationMetric(original_test_dataset, \n",
    "                                                 transf_predictions_test_dataset,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5*(TPR+TNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Plain model - without debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.866667\n",
      "Test set: Balanced classification accuracy = 0.866667\n",
      "Test set: Statistical parity difference = 0.120000\n",
      "Test set: Disparate impact = 1.250000\n",
      "Test set: Equal opportunity difference = 0.166667\n",
      "Test set: Average odds difference = 0.006410\n",
      "Test set: Theil_index = 0.092420\n",
      "Test set: False negative rate difference = -0.166667\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Model - with debiasing - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.900000\n",
      "Test set: Balanced classification accuracy = 0.900000\n",
      "Test set: Statistical parity difference = 0.160000\n",
      "Test set: Disparate impact = 1.363636\n",
      "Test set: Equal opportunity difference = 0.166667\n",
      "Test set: Average odds difference = 0.044872\n",
      "Test set: Theil_index = 0.081705\n",
      "Test set: False negative rate difference = -0.166667\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Statistical parity difference = %f\" % classified_metric_nodebiasing_test.statistical_parity_difference())\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
    "print(\"Test set: False negative rate difference = %f\" % classified_metric_nodebiasing_test.false_negative_rate_difference())\n",
    "\n",
    "display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\n",
    "print(\"Test set: Statistical parity difference = %f\" % classified_metric_debiasing_test.statistical_parity_difference())\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test.theil_index())\n",
    "print(\"Test set: False negative rate difference = %f\" % classified_metric_debiasing_test.false_negative_rate_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us break down these numbers by age to understand how these bias differs across age groups. For demonstration, we dividee all the samples into these age groups: 0-10, 10-20, 20-40, 40-60 and 60-150. For this we will create aif360 datasets using the subset of samples that fall into each of the age groups. The plot below shows how the `Equal opportunity difference` metric varies across age groups before and after applying the bias mitigating reweighing algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The favorable and unfavorable labels provided do not match the labels in the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-aaeb96bc44d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                                           \u001b[0mprivileged_groups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivileged_groups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                           \u001b[0mfavorable_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfavorable_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                           unfavorable_label=unfavorable_label)\n\u001b[0m\u001b[1;32m     20\u001b[0m     pred_dataset = dataset_wrapper(outcome=y_pred[ids], protected=p_test[ids],\n\u001b[1;32m     21\u001b[0m                                    \u001b[0munprivileged_groups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munprivileged_groups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f696fe49270c>\u001b[0m in \u001b[0;36mdataset_wrapper\u001b[0;34m(outcome, protected, unprivileged_groups, privileged_groups, favorable_label, unfavorable_label)\u001b[0m\n\u001b[1;32m     12\u001b[0m                                        \u001b[0mlabel_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outcome'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                        \u001b[0mprotected_attribute_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'race'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                        unprivileged_protected_attributes=unprivileged_groups)\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitKraken/AIF360-master/aif360/datasets/binary_label_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, favorable_label, unfavorable_label, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfavorable_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munfavorable_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBinaryLabelDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitKraken/AIF360-master/aif360/datasets/structured_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, label_names, protected_attribute_names, instance_weights_name, unprivileged_protected_attributes, privileged_protected_attributes, metadata)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0munprivileged_protected_attributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munprivileged_protected_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mprivileged_protected_attributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivileged_protected_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             metadata=metadata)\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitKraken/AIF360-master/aif360/datasets/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;34m'previous'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         })\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitKraken/AIF360-master/aif360/datasets/binary_label_dataset.py\u001b[0m in \u001b[0;36mvalidate_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m         if (set([self.favorable_label, self.unfavorable_label])\n\u001b[1;32m     45\u001b[0m                 != set(self.labels.ravel())):\n\u001b[0;32m---> 46\u001b[0;31m             raise ValueError(\"The favorable and unfavorable labels provided do \"\n\u001b[0m\u001b[1;32m     47\u001b[0m                              \"not match the labels in the dataset.\")\n",
      "\u001b[0;31mValueError\u001b[0m: The favorable and unfavorable labels provided do not match the labels in the dataset."
     ]
    }
   ],
   "source": [
    "# Metrics sliced by age\n",
    "age_range_intervals = [0, 10, 20, 40, 60, 150]\n",
    "nodebiasing_perf = []\n",
    "debiasing_perf = []\n",
    "\n",
    "for idx in range(len(age_range_intervals)-1):\n",
    "    start = age_range_intervals[idx]\n",
    "    end = age_range_intervals[idx+1]\n",
    "    ids = np.where((age_test>start) & (age_test<end))\n",
    "    true_dataset = dataset_wrapper(outcome=y_test[ids], protected=p_test[ids],\n",
    "                                   unprivileged_groups=unprivileged_groups,\n",
    "                                   privileged_groups=privileged_groups,\n",
    "                                   favorable_label=favorable_label,\n",
    "                                unfavorable_label=unfavorable_label)\n",
    "    transf_pred_dataset = dataset_wrapper(outcome=y_pred_transf[ids], protected=p_test[ids],\n",
    "                                          unprivileged_groups=unprivileged_groups,\n",
    "                                          privileged_groups=privileged_groups,\n",
    "                                          favorable_label=favorable_label,\n",
    "                                          unfavorable_label=unfavorable_label)\n",
    "    pred_dataset = dataset_wrapper(outcome=y_pred[ids], protected=p_test[ids],\n",
    "                                   unprivileged_groups=unprivileged_groups,\n",
    "                                   privileged_groups=privileged_groups,\n",
    "                                   favorable_label=favorable_label,\n",
    "                                   unfavorable_label=unfavorable_label)\n",
    " \n",
    "    classified_metric_nodebiasing_test = ClassificationMetric(true_dataset, \n",
    "                                                 pred_dataset,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "    classified_metric_debiasing_test = ClassificationMetric(true_dataset, \n",
    "                                                 transf_pred_dataset,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "    nodebiasing_perf.append(classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "    debiasing_perf.append(classified_metric_debiasing_test.equal_opportunity_difference())\n",
    "\n",
    "N = len(age_range_intervals)-1\n",
    "fig, ax = plt.subplots()\n",
    "ind = np.arange(N)\n",
    "width = 0.35\n",
    "p1 = ax.bar(ind, nodebiasing_perf, width, color='r')\n",
    "p2 = ax.bar(ind + width, debiasing_perf, width,\n",
    "            color='y')\n",
    "ax.set_title('Equal opportunity difference by age group')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels([str(age_range_intervals[idx])+'-'+str(age_range_intervals[idx+1]) for idx in range(N)])\n",
    "\n",
    "ax.legend((p1[0], p2[0]), ('Before', 'After'))\n",
    "ax.autoscale_view()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "In this tutorial, we have examined fairness in the scenario of binary classification with face images. We discussed methods to process several attributes of images, outcome variables, and protected attributes and create aif360 ready dataset objects on which many bias mititation algorithms can be easily applied and fairness metrics can be easliy computed. We used the reweighing algorithm with the aim of improving the algorithmic fairness of the learned classifiers. The empirical results show slight improvement in the case of debiased model over the vanilla model. When sliced by age group, the results appear to be mixed bag and thus has scope for further improvements by considering age group while learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
